name: DEV Deploy

on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  deploy:
    runs-on: [self-hosted, stage]
    env:
      OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      LLM_MODEL: ${{ vars.LLM_MODEL }}
      LLM_BASE_URL: https://openrouter.ai/api/v1
      LLM_PROVIDER: openai_compat
      REQUIRE_LLM: "1"

    steps:
      - name: Deploy DEV from ~/app (git pull + compose up)
        shell: bash
        run: |
          set -euo pipefail

          APP_DIR="$HOME/app"
          REPO_URL="https://github.com/eberhardred42-hub/nayti-lyudey-mvp.git"

          if [ ! -d "$APP_DIR/.git" ]; then
            mkdir -p "$APP_DIR"
            git clone "$REPO_URL" "$APP_DIR"
          fi

          cd "$APP_DIR"
          git fetch --all
          git reset --hard origin/main

          # DEV ports (must not conflict with PROD)
          export FRONT_HOST_PORT=3100
          export API_HOST_PORT=8100
          export ML_HOST_PORT=8101
          export RENDER_HOST_PORT=8102
          export MINIO_HOST_PORT=9100
          export MINIO_CONSOLE_HOST_PORT=9101
          export DB_HOST_PORT=15432
          export REDIS_HOST_PORT=16379
          export S3_PRESIGN_ENDPOINT=${S3_PRESIGN_ENDPOINT:-http://localhost:9100}

          # LLM/OpenRouter passthrough (optional; empty values are OK)
          # LLM/OpenRouter must be configured (no silent mock)
          if [ "${REQUIRE_LLM:-0}" = "1" ] && [ -z "${OPENROUTER_API_KEY:-}" ]; then
            echo "ERROR: OPENROUTER_API_KEY is not set, but REQUIRE_LLM=1" >&2
            exit 1
          fi
          export LLM_PROVIDER=openai_compat
          export LLM_BASE_URL=https://openrouter.ai/api/v1
          export LLM_API_KEY="$OPENROUTER_API_KEY"
          export OPENROUTER_API_KEY="$OPENROUTER_API_KEY"
          export LLM_MODEL="${LLM_MODEL:-gpt-4o-mini}"

          docker compose -p dev -f infra/docker-compose.yml up -d --build

      - name: Ensure caddy is running (host network)
        shell: bash
        run: |
          set -euo pipefail
          APP_DIR="$HOME/app"

          if ! docker ps -a --format '{{.Names}}' | grep -qx 'caddy'; then
            docker run -d --name caddy --restart unless-stopped --network host \
              -v "$APP_DIR/Caddyfile:/etc/caddy/Caddyfile" \
              -v caddy_data:/data -v caddy_config:/config caddy:2
          else
            docker restart caddy
          fi

      - name: Smoke checks (DEV)
        shell: bash
        run: |
          set -euo pipefail
          curl -fsS https://dev.naitilyudei.ru/ >/dev/null
          curl -fsS https://dev.naitilyudei.ru/api/docs >/dev/null
          curl -fsS https://dev.naitilyudei.ru/api/health/llm | python3 - <<'PY'
          import json,sys
          j=json.load(sys.stdin)
          if (j.get('provider')=='mock') or (j.get('mode')=='mock') or (not j.get('key_present')):
            raise SystemExit(f"LLM health is not real: {j}")
          print('LLM health OK:', j.get('provider'), j.get('model'))
          PY
          cd "$HOME/app"
          docker compose -p dev -f infra/docker-compose.yml ps

      - name: Logs (only if failed)
        if: failure()
        shell: bash
        run: |
          cd "$HOME/app"
          docker logs --tail=200 caddy || true
          docker compose -p dev -f infra/docker-compose.yml logs --no-color --tail=200 || true
