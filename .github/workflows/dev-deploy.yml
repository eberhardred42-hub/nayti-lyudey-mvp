name: DEV Deploy

on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  deploy:
    runs-on: [self-hosted, stage]
    env:
      OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      LLM_MODEL: ${{ vars.LLM_MODEL }}
      LLM_BASE_URL: https://openrouter.ai/api/v1
      LLM_PROVIDER: openai_compat
      LLM_REQUIRE_KEY: "true"

    steps:
      - name: Deploy DEV from ~/app (git pull + compose up)
        shell: bash
        run: |
          set -euo pipefail

          APP_DIR="$HOME/app"
          REPO_URL="https://github.com/eberhardred42-hub/nayti-lyudey-mvp.git"

          if [ ! -d "$APP_DIR/.git" ]; then
            mkdir -p "$APP_DIR"
            git clone "$REPO_URL" "$APP_DIR"
          fi

          cd "$APP_DIR"
          git fetch --all
          git reset --hard origin/main

          # DEV ports (must not conflict with PROD)
          export FRONT_HOST_PORT=3100
          export API_HOST_PORT=8100
          export ML_HOST_PORT=8101
          export RENDER_HOST_PORT=8102
          export MINIO_HOST_PORT=9100
          export MINIO_CONSOLE_HOST_PORT=9101
          export DB_HOST_PORT=15432
          export REDIS_HOST_PORT=16379
          export S3_PRESIGN_ENDPOINT=${S3_PRESIGN_ENDPOINT:-http://localhost:9100}

          # Deterministic runtime env (no relying on old .env on server)
          RUNTIME_ENV_FILE="$APP_DIR/.env.runtime"
          if [ -z "${OPENROUTER_API_KEY:-}" ]; then
            echo "ERROR: OPENROUTER_API_KEY is not set" >&2
            exit 1
          fi
          umask 077
          {
            printf '%s\n' "LLM_BASE_URL=https://openrouter.ai/api/v1"
            printf '%s\n' "LLM_PROVIDER=openai_compat"
            printf '%s\n' "LLM_MODEL=${LLM_MODEL:-gpt-4o-mini}"
            printf '%s\n' "OPENROUTER_API_KEY=${OPENROUTER_API_KEY}"
            printf '%s\n' "LLM_REQUIRE_KEY=true"
          } >"$RUNTIME_ENV_FILE"
          chmod 600 "$RUNTIME_ENV_FILE" || true

          docker compose --env-file "$RUNTIME_ENV_FILE" -p dev -f infra/docker-compose.yml up -d --build

      - name: Ensure caddy is running (host network)
        shell: bash
        run: |
          set -euo pipefail
          APP_DIR="$HOME/app"

          if ! docker ps -a --format '{{.Names}}' | grep -qx 'caddy'; then
            docker run -d --name caddy --restart unless-stopped --network host \
              -v "$APP_DIR/Caddyfile:/etc/caddy/Caddyfile" \
              -v caddy_data:/data -v caddy_config:/config caddy:2
          else
            docker restart caddy
          fi

      - name: Smoke checks (DEV)
        shell: bash
        run: |
          set -euo pipefail
          curl -fsS https://dev.naitilyudei.ru/ >/dev/null
          curl -fsS https://dev.naitilyudei.ru/api/docs >/dev/null
          curl -fsS https://dev.naitilyudei.ru/api/health/llm | python3 - <<'PY'
          import json,sys
          j=json.load(sys.stdin)
          if not j.get('ok'):
            raise SystemExit(f"LLM health not ok: {j}")
          if j.get('provider_effective')!='openai_compat':
            raise SystemExit(f"LLM provider_effective is not openai_compat: {j}")
          if not j.get('key_present'):
            raise SystemExit(f"LLM health is not real: {j}")
          if j.get('reason')!='ok':
            raise SystemExit(f"LLM reason is not ok: {j}")
          print('LLM health OK:', j.get('provider_effective'), j.get('model'), j.get('key_source'))
          PY
          cd "$HOME/app"
          docker compose -p dev -f infra/docker-compose.yml ps

      - name: Logs (only if failed)
        if: failure()
        shell: bash
        run: |
          cd "$HOME/app"
          docker logs --tail=200 caddy || true
          docker compose -p dev -f infra/docker-compose.yml logs --no-color --tail=200 || true
